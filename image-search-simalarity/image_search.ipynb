{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import glob \n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "from PIL import Image\n",
    "import cv2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = cv2.imread(\"/Users/menghang/Desktop/ml-dev/ml-replicate/vector-db/Alejandro_Toledo_0037.jpg\")\n",
    "model_name = 'google/vit-base-patch16-224-in21k'\n",
    "model_transformer = ViTModel.from_pretrained(model_name)\n",
    "\n",
    "inputs = feature_extractor(images=test, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = \"transformer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model_transformer(**inputs)\n",
    "\n",
    "# The extracted features are in the last hidden state\n",
    "features = outputs.last_hidden_state\n",
    "\n",
    "print(features.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features[:,0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"brain\": [], \n",
    "    \"butterfly\": [],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in glob.glob(\"caltech-101/101_ObjectCategories/brain/*\"):\n",
    "    data[\"brain\"].append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in glob.glob(\"caltech-101/101_ObjectCategories/butterfly/*\"):\n",
    "    data[\"butterfly\"].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"brain\"] = data[\"brain\"][:10]\n",
    "data[\"butterfly\"] = data[\"butterfly\"][:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_data = []\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.nn as nn \n",
    "model = resnet50( weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "model = nn.Sequential(*list(model.children())[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"butterfly\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "for brain_path in data[\"brain\"]: \n",
    "    img = cv2.imread(brain_path)\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    \n",
    "\n",
    "    if arch == \"transformer\": \n",
    "        img = feature_extractor(images=img, return_tensors=\"pt\")\n",
    "        features = model_transformer(**img).last_hidden_state\n",
    "        features = features[:, 0, :][0].tolist()\n",
    "        embedding_data.append(features)\n",
    "    else:\n",
    "        transposed_img = img.transpose(2, 0, 1)\n",
    "        features = model(torch.Tensor([transposed_img]))\n",
    "        flattened_features = features.view(features.size(0), -1)[0].tolist()\n",
    "        embedding_data.append(flattened_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bf_path in data[\"butterfly\"]: \n",
    "    img = cv2.imread(bf_path)\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    \n",
    "\n",
    "    if arch == \"transformer\": \n",
    "        img = feature_extractor(images=img, return_tensors=\"pt\")\n",
    "        features = model_transformer(**img).last_hidden_state\n",
    "        features = features[:, 0, :][0].tolist()\n",
    "        embedding_data.append(features)\n",
    "    else:\n",
    "        transposed_img = img.transpose(2, 0, 1)\n",
    "        features = model(torch.Tensor([transposed_img]))\n",
    "        flattened_features = features.view(features.size(0), -1)[0].tolist()\n",
    "        embedding_data.append(flattened_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.Client()\n",
    "collection = client.get_or_create_collection(\"image-searches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(embeddings=embedding_data, ids=[f\"id-{x}\" for x in range(len(embedding_data))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caltech-101/101_ObjectCategories/brain/image_0032.jpg',\n",
       " 'caltech-101/101_ObjectCategories/brain/image_0026.jpg',\n",
       " 'caltech-101/101_ObjectCategories/brain/image_0027.jpg',\n",
       " 'caltech-101/101_ObjectCategories/brain/image_0033.jpg',\n",
       " 'caltech-101/101_ObjectCategories/brain/image_0019.jpg',\n",
       " 'caltech-101/101_ObjectCategories/brain/image_0025.jpg',\n",
       " 'caltech-101/101_ObjectCategories/brain/image_0031.jpg',\n",
       " 'caltech-101/101_ObjectCategories/brain/image_0030.jpg',\n",
       " 'caltech-101/101_ObjectCategories/brain/image_0024.jpg',\n",
       " 'caltech-101/101_ObjectCategories/brain/image_0018.jpg']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"caltech-101/101_ObjectCategories/brain/image_0032.jpg\")\n",
    "img = cv2.resize(img, (224,224))\n",
    "inputs = feature_extractor(images=img, return_tensors=\"pt\")\n",
    "features = model_transformer(**inputs).last_hidden_state\n",
    "features = features[:, 0, :][0].tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"butterfly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"caltech-101/101_ObjectCategories/butterfly/image_0032.jpg\")\n",
    "img = cv2.resize(img, (224,224))\n",
    "cv2.imshow(\"Test\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"caltech-101/101_ObjectCategories/butterfly/image_0041.jpg\")\n",
    "img = cv2.resize(img, (224,224)).transpose(2,0,1)\n",
    "\n",
    "features = model(torch.Tensor([img]))\n",
    "flattened_features = features.view(features.size(0), -1)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"caltech-101/101_ObjectCategories/brain/image_0033.jpg\")\n",
    "img = cv2.resize(img, (224,224)).transpose(2,0,1)\n",
    "features = model(torch.Tensor([img]))\n",
    "flattened_features = features.view(features.size(0), -1)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_embeddings=features, n_results=10, include=[\"distances\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['id-0',\n",
       "   'id-8',\n",
       "   'id-3',\n",
       "   'id-6',\n",
       "   'id-4',\n",
       "   'id-9',\n",
       "   'id-1',\n",
       "   'id-7',\n",
       "   'id-2',\n",
       "   'id-5']],\n",
       " 'distances': [[0.0,\n",
       "   13.09632682800293,\n",
       "   13.95283317565918,\n",
       "   14.6825590133667,\n",
       "   14.836503982543945,\n",
       "   16.509275436401367,\n",
       "   16.68521499633789,\n",
       "   17.651121139526367,\n",
       "   19.288576126098633,\n",
       "   24.872194290161133]],\n",
       " 'metadatas': None,\n",
       " 'embeddings': None,\n",
       " 'documents': None,\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['distances']}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
