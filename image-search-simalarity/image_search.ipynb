{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import glob\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"brain\": [], \n",
    "    \"butterfly\": [],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in glob.glob(\"caltech-101/101_ObjectCategories/brain/*\"):\n",
    "    data[\"brain\"].append(i)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in glob.glob(\"caltech-101/101_ObjectCategories/butterfly/*\"):\n",
    "    data[\"butterfly\"].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"brain\"] = data[\"brain\"][:10]\n",
    "data[\"butterfly\"] = data[\"butterfly\"][:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_data = []\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import torch.nn as nn \n",
    "model = resnet50( weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "model = nn.Sequential(*list(model.children())[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "for brain_path in data[\"brain\"]: \n",
    "    img = cv2.imread(brain_path)\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    \n",
    "\n",
    "    transposed_img = img.transpose(2, 0, 1)\n",
    "    features = model(torch.Tensor([transposed_img]))\n",
    "    flattened_features = features.view(features.size(0), -1)[0].tolist()\n",
    "\n",
    "    embedding_data.append(flattened_features)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bf_path in data[\"butterfly\"]: \n",
    "    img = cv2.imread(bf_path)\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    \n",
    "\n",
    "    transposed_img = img.transpose(2, 0, 1)\n",
    "    features = model(torch.Tensor([transposed_img]))\n",
    "    flattened_features = features.view(features.size(0), -1)[0].tolist()\n",
    "    embedding_data.append(flattened_features)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"brain\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.Client()\n",
    "collection = client.get_or_create_collection(\"image-searches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(embeddings=embedding_data, ids=[f\"id-{x}\" for x in range(len(embedding_data))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"caltech-101/101_ObjectCategories/butterfly/image_0041.jpg\")\n",
    "img = cv2.resize(img, (224,224)).transpose(2,0,1)\n",
    "features = model(torch.Tensor([img]))\n",
    "flattened_features = features.view(features.size(0), -1)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"caltech-101/101_ObjectCategories/brain/image_0033.jpg\")\n",
    "img = cv2.resize(img, (224,224)).transpose(2,0,1)\n",
    "features = model(torch.Tensor([img]))\n",
    "flattened_features = features.view(features.size(0), -1)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_embeddings=flattened_features, n_results=2, include=[\"embeddings\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
